{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import testDatasetUtils as tdu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import constan as constant\n",
    "import predeal\n",
    "import tools\n",
    "import DT as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "IFPrint = False\n",
    "max_iter=10\n",
    "\n",
    "def write_acc(fileName,dataSetName,acc):\n",
    "\n",
    "    accWrite = []\n",
    "    # 判断acc中是否包含某个key\n",
    "    if 'myTree_mc' in acc.keys() and len(acc['myTree_mc']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['myTree_mc']),'.3f'))\n",
    "    if 'T_median' in acc.keys() and len(acc['T_median']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['T_median']),'.3f'))\n",
    "    if 'T_mean' in acc.keys() and len(acc['T_mean']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['T_mean']),'.3f'))\n",
    "    if 'standard' in acc.keys() and len(acc['standard']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['standard']),'.3f'))\n",
    "    if 'nearestCentroid' in acc.keys() and len(acc['nearestCentroid']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['nearestCentroid']),'.3f'))\n",
    "    if 'myTree_mc_rank' in acc.keys() and len(acc['myTree_mc_rank']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['myTree_mc_rank']),'.3f'))\n",
    "    if 'T_median_rank' in acc.keys() and len(acc['T_median_rank']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['T_median_rank']),'.3f'))\n",
    "    if 'myTree_mc_edit' in acc.keys() and len(acc['myTree_mc_edit']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['myTree_mc_edit']),'.3f'))\n",
    "    if 'T_median_edit' in acc.keys() and len(acc['T_median_edit']) != 0:\n",
    "        accWrite.append(format(np.mean(acc['T_median_edit']),'.3f'))\n",
    "\n",
    "\n",
    "    # 如果文件中还没有数据，就写入数据，如果有数据，向后追加数据\n",
    "    if not os.path.exists(fileName):\n",
    "        with open(fileName, 'w') as f:\n",
    "            f.write('%s,'%(dataSetName))\n",
    "            f.write(','.join(accWrite))\n",
    "    else:\n",
    "        with open(fileName, 'a+') as f:\n",
    "            f.write('%s,'%(dataSetName))\n",
    "            f.write(','.join(accWrite))\n",
    "            \n",
    "    #向文件中添加空行\n",
    "    with open(fileName, 'a+') as f:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def contrastExperiment_numerical(X, y, curDepth=0, maxLeafSize=1, meanWay=None, maxDepth=1000000000):\n",
    "        k = 5\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=None)\n",
    "        acc = {'myTree_mc':[],'T_median':[],'T_mean':[],'standard': [], 'nearestCentroid': []}\n",
    "        X_normalized = predeal.normalization(X)\n",
    "        for train_index, test_index in skf.split(X_normalized, y):\n",
    "            X_train_normalized, X_test_normalized = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            X_train_normalized, y_train = shuffle(X_train_normalized, y_train)\n",
    "\n",
    "            constant.set_value('gl_Xtrain', X_train_normalized)\n",
    "            constant.set_value('gl_ytrain', y_train)\n",
    "            constant.set_value('gl_distances', tools.calcDistancesMetric('numerical','euclidean',X_train_normalized))\n",
    "\n",
    "            T_gini_mc = dt.DT()\n",
    "            indeices = [index for index in range(len(X_train_normalized))]\n",
    "            T_gini_mc.fit(indeices)\n",
    "            acc['myTree_mc'].append(T_gini_mc.score(X_test_normalized, y_test))\n",
    "\n",
    "            \n",
    "        print(acc)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_iris(curDepth=0, maxLeafSize=1, meanWay='MEDIAN', maxDepth=1000000000):\n",
    "    constant._init()\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    # print_data_info(dataType='numerical',fileName='iris', X=X, y=y)\n",
    "    return contrastExperiment_numerical(X, y, curDepth, maxLeafSize, meanWay, maxDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_vhstack_dispatcher() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_iris()\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mtest_iris\u001b[0;34m(curDepth, maxLeafSize, meanWay, maxDepth)\u001b[0m\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m iris\u001b[39m.\u001b[39mtarget\n\u001b[1;32m      7\u001b[0m \u001b[39m# print_data_info(dataType='numerical',fileName='iris', X=X, y=y)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mreturn\u001b[39;00m contrastExperiment_numerical(X, y, curDepth, maxLeafSize, meanWay, maxDepth)\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mcontrastExperiment_numerical\u001b[0;34m(X, y, curDepth, maxLeafSize, meanWay, maxDepth)\u001b[0m\n\u001b[1;32m     19\u001b[0m     T_gini_mc \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mDT()\n\u001b[1;32m     20\u001b[0m     indeices \u001b[39m=\u001b[39m [index \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_train_normalized))]\n\u001b[0;32m---> 21\u001b[0m     T_gini_mc\u001b[39m.\u001b[39;49mfit(indeices)\n\u001b[1;32m     22\u001b[0m     acc[\u001b[39m'\u001b[39m\u001b[39mmyTree_mc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(T_gini_mc\u001b[39m.\u001b[39mscore(X_test_normalized, y_test))\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(acc)\n",
      "File \u001b[0;32m~/study/my_research/decisionTree_Distance/DT_apriori_test/DT.py:101\u001b[0m, in \u001b[0;36mDT.fit\u001b[0;34m(self, indeices, cate, represent)\u001b[0m\n\u001b[1;32m     99\u001b[0m curRules\u001b[39m=\u001b[39m{}\n\u001b[1;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m cate \u001b[39min\u001b[39;00m cates:\n\u001b[0;32m--> 101\u001b[0m     curRules[cate]\u001b[39m=\u001b[39mapriori\u001b[39m.\u001b[39;49mmostConfRule(gl_Xtrain[data[cate]],gl_ytrain[data[cate]])\n\u001b[1;32m    102\u001b[0m rules\u001b[39m=\u001b[39mcurRules\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    104\u001b[0m \u001b[39m# # step2:初始化中心点\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m# curRepresents = {}\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m# for cate in cates:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39m# step3:计算初始分支结果并暂存\u001b[39;00m\n",
      "File \u001b[0;32m~/study/my_research/decisionTree_Distance/DT_apriori_test/apriori.py:247\u001b[0m, in \u001b[0;36mmostConfRule\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m# frequent_one_itemsets = findFrequentOneItemsets(dataset, min_sup)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# frequent_one_itemsets = findFrequentOneItemsets(dataset[:,-1],min_sup)\u001b[39;00m\n\u001b[1;32m    245\u001b[0m frequent_result_itemsets \u001b[39m=\u001b[39m findFrequentOneItemsets(y, min_sup)\n\u001b[0;32m--> 247\u001b[0m frequent_itemsets \u001b[39m=\u001b[39m apriori(np\u001b[39m.\u001b[39;49mhstack(X,[y]), frequent_one_itemsets,frequent_result_itemsets, min_sup)\n\u001b[1;32m    249\u001b[0m rules_list \u001b[39m=\u001b[39m associationRules(frequent_itemsets, frequent_result_itemsets,min_conf)\n\u001b[1;32m    251\u001b[0m \u001b[39m# printRules(frequent_itemsets, rules_list, len(dataset), min_sup, min_conf)\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _vhstack_dispatcher() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "test_iris()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
